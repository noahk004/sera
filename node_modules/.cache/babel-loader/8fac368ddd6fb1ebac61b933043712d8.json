{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { FusedBatchNorm } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { xAs4D } from './batchnorm_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Batch normalization.\n *\n * As described in\n * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).\n *\n * Mean, variance, scale, and offset can be of two shapes:\n *   - The same shape as the input.\n *   - In the common case, the depth dimension is the last dimension of x, so\n *     the values would be a `tf.Tensor1D` of shape [depth].\n *\n * Also available are stricter rank-specific methods with the same signature\n * as this method that assert that parameters passed are of given rank\n *   - `tf.batchNorm2d`\n *   - `tf.batchNorm3d`\n *   - `tf.batchNorm4d`\n *\n * @param x The input Tensor.\n * @param mean A mean Tensor.\n * @param variance A variance Tensor.\n * @param offset An offset Tensor.\n * @param scale A scale Tensor.\n * @param varianceEpsilon A small float number to avoid dividing by 0.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction batchNorm_(x, mean, variance, offset, scale, varianceEpsilon) {\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n  const $x = convertToTensor(x, 'x', 'batchNorm');\n  const $mean = convertToTensor(mean, 'mean', 'batchNorm');\n  const $variance = convertToTensor(variance, 'variance', 'batchNorm');\n  let $scale;\n  if (scale != null) {\n    $scale = convertToTensor(scale, 'scale', 'batchNorm');\n  }\n  let $offset;\n  if (offset != null) {\n    $offset = convertToTensor(offset, 'offset', 'batchNorm');\n  }\n  util.assert($mean.rank === $variance.rank, () => 'Batch normalization gradient requires mean and variance to have ' + 'equal ranks.');\n  util.assert($offset == null || $mean.rank === $offset.rank, () => 'Batch normalization gradient requires mean and offset to have ' + 'equal ranks.');\n  util.assert($scale == null || $mean.rank === $scale.rank, () => 'Batch normalization gradient requires mean and scale to have ' + 'equal ranks.');\n  const x4D = xAs4D($x);\n  const inputs = {\n    x: x4D,\n    scale: $scale,\n    offset: $offset,\n    mean: $mean,\n    variance: $variance\n  };\n  const attrs = {\n    varianceEpsilon\n  };\n  // tslint:disable-next-line: no-unnecessary-type-assertion\n  const res = ENGINE.runKernel(FusedBatchNorm, inputs, attrs);\n  return reshape(res, $x.shape);\n}\nexport const batchNorm = /* @__PURE__ */op({\n  batchNorm_\n});","map":{"version":3,"sources":["../../../../../../tfjs-core/src/ops/batchnorm.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAM,QAAO,WAAW;AAChC,SAAQ,cAAc,QAAkD,iBAAiB;AAIzF,SAAQ,eAAe,QAAO,oBAAoB;AAElD,OAAO,KAAK,IAAI,MAAM,SAAS;AAE/B,SAAQ,KAAK,QAAO,kBAAkB;AACtC,SAAQ,EAAE,QAAO,aAAa;AAC9B,SAAQ,OAAO,QAAO,WAAW;AAEjC;;;;;;;;;;;;;;;;;;;;;;;;;AAyBG;AACH,SAAS,UAAU,CACf,CAAuB,EAAE,IAAmC,EAC5D,QAAuC,EACvC,MAAsC,EACtC,KAAqC,EACrC,eAAwB,EAAA;EAC1B,IAAI,eAAe,IAAI,IAAI,EAAE;IAC3B,eAAe,GAAG,KAAK;EACxB;EACD,MAAM,EAAE,GAAG,eAAe,CAAC,CAAC,EAAE,GAAG,EAAE,WAAW,CAAC;EAC/C,MAAM,KAAK,GAAG,eAAe,CAAC,IAAI,EAAE,MAAM,EAAE,WAAW,CAAC;EACxD,MAAM,SAAS,GAAG,eAAe,CAAC,QAAQ,EAAE,UAAU,EAAE,WAAW,CAAC;EACpE,IAAI,MAA0B;EAC9B,IAAI,KAAK,IAAI,IAAI,EAAE;IACjB,MAAM,GAAG,eAAe,CAAC,KAAK,EAAE,OAAO,EAAE,WAAW,CAAC;EACtD;EACD,IAAI,OAA2B;EAC/B,IAAI,MAAM,IAAI,IAAI,EAAE;IAClB,OAAO,GAAG,eAAe,CAAC,MAAM,EAAE,QAAQ,EAAE,WAAW,CAAC;EACzD;EAED,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,IAAI,KAAK,SAAS,CAAC,IAAI,EAC7B,MAAM,kEAAkE,GACpE,cAAc,CAAC;EACvB,IAAI,CAAC,MAAM,CACP,OAAO,IAAI,IAAI,IAAI,KAAK,CAAC,IAAI,KAAK,OAAO,CAAC,IAAI,EAC9C,MAAM,gEAAgE,GAClE,cAAc,CAAC;EACvB,IAAI,CAAC,MAAM,CACP,MAAM,IAAI,IAAI,IAAI,KAAK,CAAC,IAAI,KAAK,MAAM,CAAC,IAAI,EAC5C,MAAM,+DAA+D,GACjE,cAAc,CAAC;EAEvB,MAAM,GAAG,GAAa,KAAK,CAAC,EAAE,CAAC;EAE/B,MAAM,MAAM,GAAyB;IACnC,CAAC,EAAE,GAAG;IACN,KAAK,EAAE,MAAM;IACb,MAAM,EAAE,OAAO;IACf,IAAI,EAAE,KAAK;IACX,QAAQ,EAAE;GACX;EAED,MAAM,KAAK,GAAwB;IAAC;EAAe,CAAC;EAEpD;EACA,MAAM,GAAG,GAAG,MAAM,CAAC,SAAS,CACZ,cAAc,EAAE,MAAmC,EACnD,KAAgC,CAAc;EAE9D,OAAO,OAAO,CAAC,GAAG,EAAE,EAAE,CAAC,KAAK,CAAC;AAC/B;AAEA,OAAO,MAAM,SAAS,GAAG,eAAgB,EAAE,CAAC;EAAC;AAAU,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {FusedBatchNorm, FusedBatchNormAttrs, FusedBatchNormInputs} from '../kernel_names';\nimport {NamedAttrMap} from '../kernel_registry';\nimport {Tensor, Tensor1D, Tensor4D} from '../tensor';\nimport {NamedTensorMap} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {Rank, TensorLike} from '../types';\nimport * as util from '../util';\n\nimport {xAs4D} from './batchnorm_util';\nimport {op} from './operation';\nimport {reshape} from './reshape';\n\n/**\n * Batch normalization.\n *\n * As described in\n * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).\n *\n * Mean, variance, scale, and offset can be of two shapes:\n *   - The same shape as the input.\n *   - In the common case, the depth dimension is the last dimension of x, so\n *     the values would be a `tf.Tensor1D` of shape [depth].\n *\n * Also available are stricter rank-specific methods with the same signature\n * as this method that assert that parameters passed are of given rank\n *   - `tf.batchNorm2d`\n *   - `tf.batchNorm3d`\n *   - `tf.batchNorm4d`\n *\n * @param x The input Tensor.\n * @param mean A mean Tensor.\n * @param variance A variance Tensor.\n * @param offset An offset Tensor.\n * @param scale A scale Tensor.\n * @param varianceEpsilon A small float number to avoid dividing by 0.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction batchNorm_<R extends Rank>(\n    x: Tensor<R>|TensorLike, mean: Tensor<R>|Tensor1D|TensorLike,\n    variance: Tensor<R>|Tensor1D|TensorLike,\n    offset?: Tensor<R>|Tensor1D|TensorLike,\n    scale?: Tensor<R>|Tensor1D|TensorLike,\n    varianceEpsilon?: number): Tensor<R> {\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n  const $x = convertToTensor(x, 'x', 'batchNorm');\n  const $mean = convertToTensor(mean, 'mean', 'batchNorm');\n  const $variance = convertToTensor(variance, 'variance', 'batchNorm');\n  let $scale: Tensor<R>|Tensor1D;\n  if (scale != null) {\n    $scale = convertToTensor(scale, 'scale', 'batchNorm');\n  }\n  let $offset: Tensor<R>|Tensor1D;\n  if (offset != null) {\n    $offset = convertToTensor(offset, 'offset', 'batchNorm');\n  }\n\n  util.assert(\n      $mean.rank === $variance.rank,\n      () => 'Batch normalization gradient requires mean and variance to have ' +\n          'equal ranks.');\n  util.assert(\n      $offset == null || $mean.rank === $offset.rank,\n      () => 'Batch normalization gradient requires mean and offset to have ' +\n          'equal ranks.');\n  util.assert(\n      $scale == null || $mean.rank === $scale.rank,\n      () => 'Batch normalization gradient requires mean and scale to have ' +\n          'equal ranks.');\n\n  const x4D: Tensor4D = xAs4D($x);\n\n  const inputs: FusedBatchNormInputs = {\n    x: x4D,\n    scale: $scale,\n    offset: $offset,\n    mean: $mean,\n    variance: $variance\n  };\n\n  const attrs: FusedBatchNormAttrs = {varianceEpsilon};\n\n  // tslint:disable-next-line: no-unnecessary-type-assertion\n  const res = ENGINE.runKernel(\n                  FusedBatchNorm, inputs as unknown as NamedTensorMap,\n                  attrs as unknown as NamedAttrMap) as Tensor<R>;\n\n  return reshape(res, $x.shape);\n}\n\nexport const batchNorm = /* @__PURE__ */ op({batchNorm_});\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}