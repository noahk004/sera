{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { keep, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class SGDOptimizer extends Optimizer {\n  constructor(learningRate) {\n    super();\n    this.learningRate = learningRate;\n    this.setLearningRate(learningRate);\n  }\n  applyGradients(variableGradients) {\n    const varNames = Array.isArray(variableGradients) ? variableGradients.map(v => v.name) : Object.keys(variableGradients);\n    varNames.forEach((name, i) => {\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n      const value = ENGINE.registeredVariables[name];\n      tidy(() => {\n        const newValue = add(mul(this.c, gradient), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n  /**\n   * Sets the learning rate of the optimizer.\n   */\n  setLearningRate(learningRate) {\n    this.learningRate = learningRate;\n    if (this.c != null) {\n      this.c.dispose();\n    }\n    this.c = keep(scalar(-learningRate));\n  }\n  dispose() {\n    this.c.dispose();\n  }\n  async getWeights() {\n    return [await this.saveIterations()];\n  }\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    if (weightValues.length !== 0) {\n      throw new Error('SGD optimizer does not have settable weights.');\n    }\n  }\n  getConfig() {\n    return {\n      'learningRate': this.learningRate\n    };\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate']);\n  }\n}\n/** @nocollapse */\nSGDOptimizer.className = 'SGD'; // Note: Name matters for Python compatibility.\nregisterClass(SGDOptimizer);","map":{"version":3,"sources":["../../../../../../tfjs-core/src/optimizers/sgd_optimizer.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAM,QAAO,WAAW;AAChC,SAAQ,IAAI,EAAE,IAAI,QAAO,YAAY;AACrC,SAAQ,GAAG,QAAO,YAAY;AAC9B,SAAQ,GAAG,QAAO,YAAY;AAC9B,SAAQ,MAAM,QAAO,eAAe;AACpC,SAAoB,aAAa,QAA8C,kBAAkB;AAIjG,SAAQ,SAAS,QAAO,aAAa;AAErC;AACA,OAAM,MAAO,YAAa,SAAQ,SAAS,CAAA;EAKzC,WAAA,CAAsB,YAAoB,EAAA;IACxC,KAAK,CAAA,CAAE;IADa,IAAA,CAAA,YAAY,GAAZ,YAAY;IAEhC,IAAI,CAAC,eAAe,CAAC,YAAY,CAAC;EACpC;EAEA,cAAc,CAAC,iBAA+C,EAAA;IAC5D,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,GAC7C,iBAAiB,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,GAClC,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC;IAClC,QAAQ,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,KAAI;MAC3B,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,GAC7C,iBAAiB,CAAC,CAAC,CAAC,CAAC,MAAM,GAC3B,iBAAiB,CAAC,IAAI,CAAC;MAC3B,IAAI,QAAQ,IAAI,IAAI,EAAE;QACpB;MACD;MACD,MAAM,KAAK,GAAG,MAAM,CAAC,mBAAmB,CAAC,IAAI,CAAC;MAC9C,IAAI,CAAC,MAAK;QACR,MAAM,QAAQ,GAAG,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,EAAE,QAAQ,CAAC,EAAE,KAAK,CAAC;QAClD,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC;MACxB,CAAC,CAAC;IACJ,CAAC,CAAC;IACF,IAAI,CAAC,mBAAmB,CAAA,CAAE;EAC5B;EAEA;;AAEG;EACH,eAAe,CAAC,YAAoB,EAAA;IAClC,IAAI,CAAC,YAAY,GAAG,YAAY;IAChC,IAAI,IAAI,CAAC,CAAC,IAAI,IAAI,EAAE;MAClB,IAAI,CAAC,CAAC,CAAC,OAAO,CAAA,CAAE;IACjB;IACD,IAAI,CAAC,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,YAAY,CAAC,CAAC;EACtC;EAEA,OAAO,CAAA,EAAA;IACL,IAAI,CAAC,CAAC,CAAC,OAAO,CAAA,CAAE;EAClB;EAEA,MAAM,UAAU,CAAA,EAAA;IACd,OAAO,CAAC,MAAM,IAAI,CAAC,cAAc,CAAA,CAAE,CAAC;EACtC;EAEA,MAAM,UAAU,CAAC,YAA2B,EAAA;IAC1C,YAAY,GAAG,MAAM,IAAI,CAAC,iBAAiB,CAAC,YAAY,CAAC;IACzD,IAAI,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;MAC7B,MAAM,IAAI,KAAK,CAAC,+CAA+C,CAAC;IACjE;EACH;EAEA,SAAS,CAAA,EAAA;IACP,OAAO;MAAC,cAAc,EAAE,IAAI,CAAC;IAAY,CAAC;EAC5C;EAEA;EACA,OAAO,UAAU,CACb,GAA+B,EAAE,MAAkB,EAAA;IACrD,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC;EACxC;;AA/DA;AACO,YAAA,CAAA,SAAS,GAAG,KAAK,CAAC,CAAE;AAgE7B,aAAa,CAAC,YAAY,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {keep, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {mul} from '../ops/mul';\nimport {scalar} from '../ops/scalar';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {Scalar} from '../tensor';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\nimport {Optimizer} from './optimizer';\n\n/** @doclink Optimizer */\nexport class SGDOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'SGD';  // Note: Name matters for Python compatibility.\n  protected c: Scalar;\n\n  constructor(protected learningRate: number) {\n    super();\n    this.setLearningRate(learningRate);\n  }\n\n  applyGradients(variableGradients: NamedTensorMap|NamedTensor[]) {\n    const varNames = Array.isArray(variableGradients) ?\n        variableGradients.map(v => v.name) :\n        Object.keys(variableGradients);\n    varNames.forEach((name, i) => {\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n      const value = ENGINE.registeredVariables[name];\n      tidy(() => {\n        const newValue = add(mul(this.c, gradient), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  /**\n   * Sets the learning rate of the optimizer.\n   */\n  setLearningRate(learningRate: number) {\n    this.learningRate = learningRate;\n    if (this.c != null) {\n      this.c.dispose();\n    }\n    this.c = keep(scalar(-learningRate));\n  }\n\n  dispose() {\n    this.c.dispose();\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    return [await this.saveIterations()];\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    if (weightValues.length !== 0) {\n      throw new Error('SGD optimizer does not have settable weights.');\n    }\n  }\n\n  getConfig(): ConfigDict {\n    return {'learningRate': this.learningRate};\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate']);\n  }\n}\nregisterClass(SGDOptimizer);\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}