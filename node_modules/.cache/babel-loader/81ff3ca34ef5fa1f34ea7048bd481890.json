{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Softmax } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the softmax normalized vector given the logits.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction softmax_(logits, dim = -1) {\n  const $logits = convertToTensor(logits, 'logits', 'softmax', 'float32');\n  if (dim === -1) {\n    dim = $logits.rank - 1;\n  }\n  if (dim !== $logits.rank - 1) {\n    throw Error('Softmax along a non-last dimension is not yet supported. ' + `Logits was rank ${$logits.rank} and dim was ${dim}`);\n  }\n  const inputs = {\n    logits: $logits\n  };\n  const attrs = {\n    dim\n  };\n  return ENGINE.runKernel(Softmax, inputs, attrs);\n}\nexport const softmax = /* @__PURE__ */op({\n  softmax_\n});","map":{"version":3,"sources":["../../../../../../tfjs-core/src/ops/softmax.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAM,QAAO,WAAW;AAChC,SAAQ,OAAO,QAAoC,iBAAiB;AAIpE,SAAQ,eAAe,QAAO,oBAAoB;AAGlD,SAAQ,EAAE,QAAO,aAAa;AAE9B;;;;;;;;;;;;;;;;;;;;AAoBG;AACH,SAAS,QAAQ,CAAmB,MAAoB,EAAE,GAAG,GAAG,CAAC,CAAC,EAAA;EAChE,MAAM,OAAO,GAAG,eAAe,CAAC,MAAM,EAAE,QAAQ,EAAE,SAAS,EAAE,SAAS,CAAC;EAEvE,IAAI,GAAG,KAAK,CAAC,CAAC,EAAE;IACd,GAAG,GAAG,OAAO,CAAC,IAAI,GAAG,CAAC;EACvB;EACD,IAAI,GAAG,KAAK,OAAO,CAAC,IAAI,GAAG,CAAC,EAAE;IAC5B,MAAM,KAAK,CACP,2DAA2D,GAC3D,mBAAmB,OAAO,CAAC,IAAI,gBAAgB,GAAG,EAAE,CAAC;EAC1D;EAED,MAAM,MAAM,GAAkB;IAAC,MAAM,EAAE;EAAO,CAAC;EAC/C,MAAM,KAAK,GAAiB;IAAC;EAAG,CAAC;EAEjC,OAAO,MAAM,CAAC,SAAS,CACnB,OAAO,EAAE,MAAmC,EAC5C,KAAgC,CAAC;AACvC;AAEA,OAAO,MAAM,OAAO,GAAG,eAAgB,EAAE,CAAC;EAAC;AAAQ,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {Softmax, SoftmaxAttrs, SoftmaxInputs} from '../kernel_names';\nimport {NamedAttrMap} from '../kernel_registry';\nimport {Tensor} from '../tensor';\nimport {NamedTensorMap} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {TensorLike} from '../types';\n\nimport {op} from './operation';\n\n/**\n * Computes the softmax normalized vector given the logits.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction softmax_<T extends Tensor>(logits: T|TensorLike, dim = -1): T {\n  const $logits = convertToTensor(logits, 'logits', 'softmax', 'float32');\n\n  if (dim === -1) {\n    dim = $logits.rank - 1;\n  }\n  if (dim !== $logits.rank - 1) {\n    throw Error(\n        'Softmax along a non-last dimension is not yet supported. ' +\n        `Logits was rank ${$logits.rank} and dim was ${dim}`);\n  }\n\n  const inputs: SoftmaxInputs = {logits: $logits};\n  const attrs: SoftmaxAttrs = {dim};\n\n  return ENGINE.runKernel(\n      Softmax, inputs as unknown as NamedTensorMap,\n      attrs as unknown as NamedAttrMap);\n}\n\nexport const softmax = /* @__PURE__ */ op({softmax_});\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}